import data.processed
import modules.model

# 训练步数与优化器超参数
train.iterations=50000  # 总训练迭代步数
train.learning_rate=0.0003  # AdamW 学习率
train.weight_decay=0.035  # AdamW 权重衰减
train.batch_size=256  # 每步 batch 大小
#train.amp=True 混合精度

# RQ‑VAE tokenizer 相关超参（应与预训练权重一致）
train.vae_input_dim=768  # 物品特征向量维度（Amazon 文本编码）
train.vae_hidden_dims=[512, 256, 128]  # RQ‑VAE 编码器/解码器隐藏层
train.vae_embed_dim=32  # RQ‑VAE 隐空间维度
train.vae_n_cat_feats=0  # 类别特征数量（Amazon 为 0，纯连续）
train.vae_codebook_size=256  # 每层 codebook 大小

# 日志与可视化
train.wandb_logging=True  # 是否启用 W&B 记录
# https://api.wandb.ai/links/botta-edoardo-carnegie-mellon-university/ilggivkz, https://api.wandb.ai/links/botta-edoardo-carnegie-mellon-university/ufe656js
train.pretrained_rqvae_path="trained_models/rqvae_amazon_beauty/checkpoint_399999.pt"  # 预训练 RQ‑VAE 权重路径（供 tokenizer 使用）
# Best run: https://api.wandb.ai/links/botta-edoardo-carnegie-mellon-university/xb46itxc
# Checkpoint run: https://api.wandb.ai/links/botta-edoardo-carnegie-mellon-university/6m2fkl1y
# train.pretrained_decoder_path="trained_models/transformer_amazon_beauty/checkpoint_199999.pt"

# 数据集路径与处理选项
train.save_dir_root="out/decoder/amazon/"  # checkpoint 保存目录
train.dataset_folder="dataset/amazon"  # 数据集根目录
train.dataset=%data.processed.RecDataset.AMAZON  # 数据集枚举（当前仅支持 Amazon）
train.force_dataset_process=False  # 是否强制重新处理数据
train.dataset_split="beauty"  # Amazon 子数据集（beauty/sports/toys）

# 评估频率（步）
train.full_eval_every=10000  # 完整评估频率（含生成 + Top‑K）
train.partial_eval_every=5000  # 部分评估频率（只算 loss）

# Decoder Transformer 超参
train.dropout_p=0.15  # Dropout 概率
train.attn_heads=8  # 注意力头数
train.attn_embed_dim=512  # 注意力内部维度
train.attn_layers=8  # Transformer 层数（编码器/解码器各一半）
train.decoder_embed_dim=128  # 语义 ID embedding 维度
train.model_jagged_mode=True  # 是否使用 jagged（变长）注意力实现
#train.save_name_prefix="checkpoint"  # 保存权重文件名前缀
train.save_name_prefix="ampon_it50000"  # 保存权重文件名前缀
